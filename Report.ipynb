{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Report of Project 05: Implementation and Evaluation of Machine Learning (Support Vector Machine) Segmentation\n",
    "## Data analysis project - B.Sc. Molecular Biotechnology Heidelberg University\n",
    "### 19.07.21\n",
    "### Authors: Michelle Emmert, Juan Andre Hamdan, Laura Sanchis Pla and Gloria Timm\n",
    "\n",
    "# Abstract\n",
    "\n",
    "Support Vector Machines, also known as SVMs, are supervised learning models with associated learning algorithms that\n",
    "analyze data for classification and regression analysis.\n",
    "\n",
    "Their theoretical foundations and their experimental success encourage further research on their characteristics, as well\n",
    "as their further use. This report presents the summary of a project that implements and evaluates a Support Vector\n",
    "Machine for segmentation of cell nuclei using the dice score, synthetically generated images, and a variety of different\n",
    "pre-processing and preparation methods.\n",
    "\n",
    "\n",
    "\n",
    "# Table of contents\n",
    "**1. Introduction** <br>\n",
    "**2. Our Data** <br>\n",
    "**3. Our algorithms pipeline** <br>\n",
    "**4. Pre-processing** <br>\n",
    "**5. Support Vector Machine** <br>\n",
    "**5.1 The Mathematical Background** <br>\n",
    "**5.2 Data reduction: Principle component analysis and Tiles** <br>\n",
    "**5.3 Stochastic gradient decent to minimize the loss gradient** <br>\n",
    "**5.4 K-Fold Cross validation** <br>\n",
    "**5.5 Implementing the Support Vector Machine** <br>\n",
    "**6. Evaluation using the Dice coefficient** <br>\n",
    "**6.1 The Theory behind the Dice Coefficient** <br>\n",
    "**6.2.1 Implementing the Dice Coefficient** <br>\n",
    "**6.2.1 Unittesting the Dice Coefficient** <br>\n",
    "**6.3 Synthetic Images** <br>\n",
    "**6.3.1 Definition and Goal** <br>\n",
    "**6.3.2 Image composition** <br>\n",
    "**6.3.3 Domain randomization** <br>\n",
    "**7. Results** <br>\n",
    "**8. Discussion** <br>\n",
    "**9. Bibliography**\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "Image segmentation is a process, during which important features of a picture are extracted, to aid analysis and retrieval\n",
    "of information. This is done by assigning labels to all pixels of the image. Pixels sharing defined traits, are allocated the same label.\n",
    "Nuclei segmentation is a subform of image segmentation. In medicine nuclei segmentation is used to classify and\n",
    "grade cancer based on cancer histopathology. Today, cancer grading is still often done manually by visual analysis of tissue samples.\n",
    "Problematic with this method is its inter- and intra-observer variability regarding the gradings quality, its low reproducibility\n",
    "as well as its great time consumption. Cancer grading is an essential step in quantifying the degree of malignancy and thus\n",
    "to predict patient prognosis and prescribe a treatment.\n",
    "\n",
    "Using a support vector machine is one way nuclei segmentation can be achieved.\n",
    "We use the support vector machine to differentiate between foreground and background pixels. Foreground pixels are\n",
    "subsequently labeled as nuclei and displayed white. Background pixels are colored black. This also allows counting of the nuclei in the end.\n",
    "To efficiently segment our images with a support vector machine, they first need to undergo preprocessing, to enhance\n",
    "picture quality. After segmentation, we want to evaluate the quality of our segmentation, which we will be using the Dice\n",
    "score for.\n",
    "\n",
    "# 2. Our Data\n",
    "Our data consists of 3 datasets with a total of 28 very different kinds of images all showing nuclei.\n",
    "The pictures of the first dataset show GFP-transfected GOWT1 mouse embryonic stem cells. The second set of images show\n",
    "Histone-2B-GFP expressing HeLa cells, while our third set consists of pictures of mouse embryonic fibroblasts, in which\n",
    "CD-antigens were tagged with enhanced-GFP.\n",
    "Though all images are microscopic images of nuclei, the three sets vary greatly in other features.\n",
    "The images are of different formats (1024 x 1024, 1100 x 700, 1344 x 1024), were all acquired differently and display\n",
    "different numbers of nuclei: within a range of 15-65 nuclei per image.\n",
    "Additionally, all images pose diverse challenges for our image analysis.\n",
    "Firstly brightness and resolution of the images differ. On top of that every set of images has its individual challenges\n",
    "like white flashes, clustering of nuclei or nuclei leaving the image or undergoing mitosis.\n",
    "\n",
    "# 3. Our algorithm's pipeline\n",
    "Using the data described in #2 we apply either a watershed filter or a gaussian filter (see #4) to improve the quality of our images.\n",
    "To evaluate the usefulness of these filtering methods, we use the un-preprocessed pictures for one SVM-run as well. The filtered image is further\n",
    "prepared for the SVM by undergoing one of two slightly different principle-component analysis and/or by being cut into a\n",
    "specific number of tiles (see #5.2). PCA reduces the features of our images to those who explain the most variance. It thus is a\n",
    "selective way of data reduction.\n",
    "By spliting the images into tiles, the total amount of data is reduced non-selectively. After processing the images either\n",
    "with PCA and/or the tiles algorithm, all intensity values are normalized.\n",
    "Additionally, as label-assignation, the ground truth images are thresholded to obtain black-and-white-only images.\n",
    "The next step is the segmentation using the Support Vector Machine (see #5).\n",
    "At last, the segmentation results are compared by calculating and comparing the dice score with the help of the ground truth images (see #6).\n",
    "\n",
    "# 4. Pre-processing\n",
    "We implemented two different pre-processing methods in our SVM algorithm in order to improve the quality of our raw\n",
    "images, and therefore achieve better segmentation results.\n",
    "The first method we used is a Gaussian filter. It is a technique that was shown to be particularly useful to filter images with a lot\n",
    "of noise. This is the case, since the results of the filtering show a relative independence on the variance value of the Gaussian\n",
    "kernel (Gedraite, E. et al. 2011). Problematic is, that the use of a Gaussian filter as pre-processing for edge detection could also give\n",
    "rise to edge position displacement, edges vanishing, and phantom edges (Deng, G. et al. 1993).\n",
    "\n",
    "The second pre-processing step is a form of super-pixel segmentation.\n",
    "The principle behind this type of segmentation is to group similar pixels from a local region together and change\n",
    "their intensities to a common value. For our approach, we used Watershed, a gradient-ascend-based super pixel\n",
    "algorithm.\n",
    "It works iteratively to refine the super-pixel clusters until a certain criteria is achieved. The image can be viewed\n",
    "as a topographic surface with high intensity peaks and low intensity valleys. First, every isolated valley is\n",
    "labeled differently. As the intensity level rises, different valleys start to merge together. To avoid this from happening,\n",
    "barriers are built at the merging points. This is repeated, until all peaks are labeled. The barriers subsequently result in\n",
    "the final segmentation result.\n",
    "\n",
    "# 5. Support Vector Machine\n",
    "In 1992, Vapnik and coworkers proposed a supervised algorithm, developed from statistical learning, to solve classification\n",
    "problems (Vapnik et al., 1992). Since then, their machine learning method evolved into what is now known as\n",
    "Support Vector Machines (SVM): a class of algorithms for classification, regression and other applications that\n",
    "represents the current state of the art in the field (Suthaharan, 2016; Guanasekaran, 2010; Christianini and Ricci, 2008).\n",
    "\n",
    "By providing a training data set with binary labels, the SVM is able to learn how to classify data points using certain\n",
    "features. This capability can subsequently be used to classify new data, called test data, using its features (Thai et. al, 2012).\n",
    "SVMs have been successfully applied to a number of applications, ranging from the time series prediction and face recognition\n",
    "to biological data processing for medical diagnosis (Evgeniou, 2001).\n",
    "In image processing, support vector machines are used for one of the classical challenges: image classification (Evgeniou, 2001).\n",
    "\n",
    "## 5.1 The Mathematical Background\n",
    "The mathematical concepts are key in understanding how Support Vector Machines work.\n",
    "The goal of an SVM is, to separate data points into two groups of provided labels with an optimal hyperplane.\n",
    "This hyperplane is described by"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(2) \\ w * x + b = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and fullfilling the following condition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(3) \\ h =\n",
    "\\left\\{\n",
    "  \\begin{aligned}\n",
    "    +1 \\ \\ if \\ \\ w⋅x_i +b≥ +1 - \\varepsilon_i\\\\\n",
    "    -1 \\ \\ if \\ \\ w⋅x_i +b< -1 - \\varepsilon_i\n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "  \\ \\ \\varepsilon \\geqq 0 \\ \\forall_i \\ ; \\ i=1...m\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "whilst for two dimensions $w = (a, -1)$, whereas $a$ is the slope of the line, and $x = (x_1, x_2)$ and represents a\n",
    "data point. $\\varepsilon$ is a variable, standing for the inaccuracy of the hyperplane. It is added to the constraint to\n",
    "prevent overfitting of the model onto the training set. Without $\\varepsilon$ the geometric margin M is called a hard margin,\n",
    "as it does not allow data points of one group to be falsely labeled as members of the other group. This does not result in\n",
    "the best model, as single falsely assigned data points, can have a lower impact on the quality of the model, than a suboptimal\n",
    "hyperplane. Therefore, $\\varepsilon$ is introduced and thereupon M is called a soft margin.\n",
    "\n",
    "To choose the optimal hyperplane we need to minimize the margin as follows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(4)\\ M = min_{i=1...m} \\ y_i(\\frac{w}{||w||}*x + \\frac{b}{||w||})\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The largest margin M out of all margins computed in our training phase, will be selected. The variables w and b are\n",
    "divided by the length of the vector w calculated with the Euclidean norm formula, as they need to be scale invariant.\n",
    "The aim is, to find the values for w and b, corresponding to the largest margin.\n",
    "\n",
    "This leads us to the following optimization problem.\n",
    "We want to maximize M:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(5) \\ max_{w,b,\\varepsilon} M\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This maximization problem is equivalent to"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(6) \\ max_{w,b,\\varepsilon} \\frac{1}{||w||} \\ + \\ \\sum^{m}_{i=1}\\varepsilon_i\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and can be rewritten as the following minimalization problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(7) \\ min_{w,b,\\varepsilon}  \\ \\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i \\\\\n",
    "subject \\ to \\ \\  y_i(w⋅x_i+b)≥1− \\varepsilon_i \\ , \\varepsilon \\geqq 0 \\ \\forall_i \\ , \\ i=1...m\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The regularization parameter C is chosen by the user and determines the weight of $\\varepsilon$.\n",
    "A larger C leads to a higher penalty for errors and therefore to a harder margin.\n",
    "\n",
    "In order to solve this constrained optimization problem, in which we want to maximize the margin while fulfilling our\n",
    "conditions or constraints, Lagrange multipliers are used. The idea behind this mathematical concept is that at the optimum,\n",
    "the gradient of our objective function is parallel or antiparallel to the gradient of the constraint function.\n",
    "Therefore, both have to be equal or a multiple of each other, which is what the Lagrange multiplier is showcasing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(8) \\ \\nabla f(x) - \\alpha \\nabla g(x) = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we insert our functions, we get the following Lagrangian function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(9) \\ f(x)= \\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i\n",
    "\\\\(10) \\ g(x) = y_i(w⋅x_i+b) - 1 + \\varepsilon_i\n",
    "\\\\(11) \\ \\mathcal{L}(w,b,\\alpha) =\\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i - \\sum^{m}_{i=1}\\alpha_i[y_i\n",
    "(w⋅x_i+b) - 1 + \\varepsilon_i]\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to solve this Lagrange problem, it is relaxed into a dual problem: The constraints are incorporated\n",
    "into the function, resulting in it only depending on the Lagrange multipliers. This facilitates the solving.\n",
    "Below, the two constraints for the dual problem are described:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(12) \\ \\nabla_w \\mathcal{L}(w,b,\\alpha) = w - \\sum^{m}_{i=1} \\alpha_i y_i x_i = 0\n",
    "\\\\ (13) \\ \\nabla_b \\mathcal{L}(w,b,\\alpha) = - \\sum^{m}_{i=1} \\alpha_i y_i = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If they are inserted into the Lagrange function, the result of the dual problem is the following:\n",
    "\\begin{equation}\n",
    "(14) \\ max_{\\alpha}  \\ \\sum^{m}_{i=1}\\alpha_i - \\frac{1}{2}\\sum^{m}_{i=1}\\sum^{m}_{j=1}\\alpha_i\\alpha_j y_i y_j x_i · x_j \\\\\n",
    "subject \\ to \\ \\ 0≤\\alpha_i≤C, i=1...m, \\sum^{m}_{i=1}\\alpha_iy_i = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above equation, it becomes clear that the maximization depends solely on the dot product of the support vectors\n",
    "$x_i · x_j$. This is an advantage when dealing with data that is not linearly separable. The 'trick' is to transform the data\n",
    "into a higher dimension, in which a separating hyperplane can be found. However, for a large dataset, calculating the transformation\n",
    "would be a very time-consuming operation. For that reason, instead of actually calculating the transformation, the Kernel trick is used.\n",
    "This means a function is used, which calculates the dot product of $x_i · x_j$ as if the two were in a higher dimension.\n",
    "\n",
    "The linear Kernel function can be described as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "K(x_i, x_j)=\\phi(x_i) · phi(x_j)\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "but it varies depending on the type of data that has to be classified. Other know and comonly used Kernel functions are:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "K(x_i, x_j)=x_i · x_j,\n",
    "\\end{equation}\n",
    "\n",
    "!!!*still missing/ has to be elaborated*!!!:\n",
    "-Wolfe dual problem\n",
    "-Kernel trick"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Data reduction: Principle component analysis and Tiles\n",
    "As the flattened images lead to an array with over a million columns, which would take the SVM a huge amount of time to process,\n",
    "we saw the need to apply a data reduction method beforehand.\n",
    "The two possibilites we used and compared, were Principal Component Analysis and cutting the image into tiles before averageing\n",
    "over each tile.\n",
    "\n",
    "Principal Component Analysis (PCA) is a mathematical procedure which uses sophisticated mathematical principles to transform\n",
    "a number of correlated variables into a smaller number of variables called principal components. This\n",
    "statistical method is frequently used in image processing for data compression, data dimension reduction,\n",
    "and data decorrelation. In PCA, the information contained in a set of data is stored with reduced dimensions,\n",
    "based on the integral projection of the data set onto a subspace, generated by a system of orthogonal axes. The reduced dimensions'\n",
    "computational content is selected so that significant data characteristics are identified with little information loss.\n",
    "Such a reduction is advantageous in several scientific applications, among which is our present implementation in image\n",
    "compression. (Ashour, A. 2015, Mudrova, M. 2005)\n",
    "PCA can thus be summarized as being a method for specific data reduction.\n",
    "\n",
    "As for PCA there is one option of preforming a PCA that does not reduce the number of features furtherly used for the SVM. This can be achieved using the scaling operation\n",
    "mentioned before. In this case, tiles is needed additionally to reduce the amount of data.\n",
    "Another possible option would be to use a PCA that reduces the amount of features to the greater minimum of the number of\n",
    "features or the number of samples. In this case, as too little PCs lead to poor results, supplementing the given data with\n",
    "synthetically generated microscopic cell images is needed.\n",
    "\n",
    "In contrast to this specific reduction of data, tiles-rendering is a simple approach that reduces data unspecifically by exploiting the fundamental properties of a problem space.\n",
    "The concept behind this method is to save computational space by splitting the image pixels into multiple sets of N x N tiles\n",
    "and calculating the average grey from each set. The final result of this averaging will be another image which\n",
    "pixels are the average intensities of the N x N pixel sets of the bigger and higher resolution, original, image (Rastar, A. 2019).\n",
    "\n",
    "## 5.3 Stochastic gradient decent to minimize the loss gradient\n",
    "In order to minimize function (7), its gradient is calculated through the Lagrange multiplicators.\n",
    "A common method to minimize this function is called Gradient Descent.\n",
    "The gradient ∇P(θ) of an objective function e.g. P(θ), which is parameterized by the model's parameters θ, is calculated.\n",
    "∇P(θ) represents the slope with the highest inclination of our function.\n",
    "During Gradient Decent, the parameters are updated in the opposite direction of the gradient. This process is repeated until a\n",
    "(local) minimum is reached, by taking steps determined beforehand by the learning rate.\n",
    "To explain differently, the direction of the slope of the surface described by P(θ) is followed downwards until the lowest\n",
    "point. (Ruder, 2017)\n",
    "Differents kinds of gradient decents mostly only differ in the amount of data they use to compute the gradient. Essentially,\n",
    "the decision between accuracy of the parameter update and the runtime has to be made.\\n\",\n",
    "As part of our SVM, we used Stochastic Gradient Descent (SGD). In contrast to the basic gradient descent, SGD does not use\\n\",\n",
    "all of the data for it's calculation, but only a randomly selected part of it, called stochastic representation (Johnson and Zhang, 2013).\n",
    "This reduces computation time significantly and makes the program faster (Johnson and Zhang, 2013).\n",
    "\n",
    "\n",
    "## 5.4 K-Fold Cross validation\n",
    "Validation is a widely used technique in data science to evaluate how stable a machine learning (ML) model is and to\n",
    "assess how well the model would generalize to new, independent data. Relevant for these two characteristics is the MLs\n",
    "ability to differentiate between relevant patterns and noise in the data available (Vabalas et. al, 2019). As a measure\n",
    "for how good the ML is able to achieve this, the bias-variance tradeoff can be used (Geman et. al, 1992; Berrar, 2019).\n",
    "Bias and variance are both sources of error in ML generalization. With increasing model complexity, bias decreases and\n",
    "variance increases monotonically (Yang et. al, 2020). In short: High bias indicates 'underfitting', high variance points\n",
    "at an 'overfitted' model (Yang et. al, 2020). 'Underfitting' describes the performance a model, which is neither able to\n",
    "classify its training data nor new data well, because it captures too little patterns. 'Overfitting' on the other hand\n",
    "refers to a model, that is overly sensitive to inherent noise and random pattern in it's training data and for that reason\n",
    "performs poorly on new data. Optimally both bias and variance could be minimized (Geman et. al, 1992; Berrar, 2019).\n",
    "In reality the right balance of both is needed to generate an optimal model (Yang et. al, 2020).\n",
    "\n",
    "One validation technique is k-fold cross validation (CV).\n",
    "In CV, the data available is split into k subsets. The data encompasses n dissimilar samples. k is a random\n",
    "integer between 1 and n. For each iteration, k-1 subsets are used as training data, while the remaining subsets are\n",
    "used to test the model and are thus part of the validation set.\n",
    "To put it differently: each data sample is part of the testing data once and part of the training data for all other\n",
    "iterations (Vabalas et. al, 2019).\n",
    "*This approach substantially reduces bias, as it uses most data points for fitting. Simultaneously variance also decreases.\n",
    "But as only one datapoint is used for testing in each iteration, higher variation in testing model effectiveness\n",
    "can occur (Berrar, 2019).*\n",
    "\n",
    "As our data consists of only 28 images, we used a special case of cross-validation called leave one out cross-validation (LOOCV).\n",
    "In this approach k equals the number of samples. So for each iteration 27 images are part of the training set, while 1 image\n",
    "acts as the validation set.\n",
    "\n",
    "## 5.5 Implementing the Support Vector Machine\n",
    "\n",
    "*TO BE DONE*\n",
    "\n",
    "# 6. Dice Coefficient\n",
    "## 6.1 The Theory behind the Dice Coefficient\n",
    "The Dice coefficient is a score to evaluate and compare the accuracy of a segmentation.\n",
    "Needed for its calculation are the segmented image, as well as a corresponding binary reference point also called\n",
    "ground truth (Bertels et al., 2019).\n",
    "As ground truth image, researchers mostly use the segmentation result of humans. We will use the ground truth images\n",
    "provided with our data sets, which we suspect to be acquired by this method.\n",
    "Using the ground truth image, the labels true positive (TP), false positive (FP) and false\n",
    "negative (FN) are assigned to each pixel of the segmented image (Menze et al., 2015).\n",
    "This information is then used to calculate the dice coefficient using formula (1):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(1) \\ dice = {\\frac{2TP}{2TP + FP + FN}} \\ \\ \\varepsilon \\ \\ [0,1]\n",
    "\\end{equation} <br>\n",
    "(Menze et al., 2015)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A dice score of 0 indicates that the ground truth and the segmentation result do not have any overlap. A dice score of 1 on the\n",
    "other hand, shows a 100% overlap of ground truth and segmented image (Bertels et al., 2019).\n",
    "\n",
    "\n",
    "## 6.2.1 Implementing the Dice Coefficient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from SVM_Segmentation.dicescore import dice_score as dicescore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2.2 Unittesting the Dice Coefficient\n",
    "To test the code for the dice coefficient, we used a frequently used method of software testing: unittests.\n",
    "Unittests are a way of validating that a specific code chunk, a unit, performs as expected and thus its result is as anticipated (Hamill, 2005).\n",
    "\n",
    "We implemented two kinds of unit tests.\n",
    "The dice coefficient of an image with itself is always 1.0. For our first unit test, we used this knowledge to test our code.\n",
    "For this first test we generated synthetic masks, black-and-white synthetic images, with which we performed the unit test.\n",
    "\n",
    "For the second unit test, we defined two random arrays, consisting only of ones and zeros.\n",
    "One array represented the segmented image, while the other served as ground truth.\n",
    "Using formula (1) we calculated the dice by hand and compared our result with our codes' output.\n",
    "In addition, we compared our code's result to the implemented\n",
    "\n",
    "## 6.3 Synthetic Images\n",
    "### 6.3.1 Definition and Goal\n",
    "The basic concept behind creating synthetic images is, to use algorithms and already available images to generate\n",
    "new ones (Dunn et al., 2019). Our first objective was to simply use these new images to test our code for the dice score.\n",
    "But while researching on this topic we realized that synthetic images have an immense potential, especially for the training\n",
    "phase of a machine learning algorithm.\n",
    "This is particularly useful as our data encompasses only 28 images, which leads to a training data set of 27 images at max.\n",
    "By expanding our training set with diverse images of good quality, we expect a more accurate model (Mayer et al., 2017).\n",
    "There are various methods for the generation of synthetic images (Ward et al., 2019). Because of the scope of our project and the\n",
    "kind of images we wanted to produce, we focused on image composition and domain randomization.\n",
    "\n",
    "### 6.3.2 Image composition\n",
    "To produce synthetic masks, a white circle is drawn on top of a black background.\n",
    "While the background stays the same throughout all images, the circle's size and position is varied.\n",
    "Our algorithm iterates through the random position and scaling generator. By doing this it produces random images, which\n",
    "can further be used as masks for our dice score, as these images are black-and-white only. Thus, their pixel intensities are either 0 or 1.\n",
    "We used these masks to test our dice score.\n",
    "\n",
    "### 6.3.3 Domain randomization\n",
    "In order to create synthetic microscopic cell images, we used a method called domain randomization.\n",
    "At first, domain randomization requires collecting various foreground images, either by separating the images from their\n",
    "backgrounds or by using images in .png format to begin with.\n",
    "These foreground images are then pasted onto different backgrounds (Tripathi, 2019).\n",
    "To obtain more variety among the resulting synthetic images, the foreground images can be modified using different contrasts,\n",
    "zooms or rotations (Ward et al., 2019; Alghonaim and Johns, 2020).\n",
    "\n",
    "We used domain randomization to generate a new set of images with cells cut from our 28 images data set.\n",
    "After rotating and scaling the cells, they were pasted at a random position onto a background, which had also been cut and\n",
    "scaled from our dataset.\n",
    "These new images were used further on to enlarge and enrich our training data set.\n",
    "\n",
    "# 7. Results\n",
    "Our goal is to determine the best combination of the different possible pre-processing and preparation settings.\n",
    "To put it differently, we want to evaluate whether at all and which specific changes to the original images enhance the\n",
    "segmentation result. These changes encompass Gauss-Filter, Watershed, Tiles and Principal Component Analysis (PCA).\n",
    "For a more precise evaluation, we will use the dice score function to compare the final, segmented, images. The comparisons\n",
    "we did are the following:\n",
    "1) for the pre-processing: with Gauss-Filter vs. without, with Watershed vs. without, with Watershed vs. with Gauss\n",
    "2) for the SVM-preparation: with Tiles and PCA (without feature reduction (fr)) vs. with Tiles, with PCA (with fr) vs. with Tiles,\n",
    "with PCA (with fr) vs. with Tiles and PCA.\n",
    "\n",
    "# 8. Discussion\n",
    "\n",
    "*TO BE DONE*\n",
    "\n",
    "# 9. Bibliography\n",
    "\n",
    "Alghonaim, R., Johns, E. (2020).Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. CoRR.\n",
    "\n",
    "Berrar, D. (2019). Cross-validation. Data Science Laboratory, Tokyo Institute of Technology.\n",
    "\n",
    "Bertels, J., Eelbode, T., Berman, M., Vandermeulen D., Maes F., Bisschops, R., Blaschko, M. (2020).\n",
    "Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index.\n",
    "IEEE Trans Med Imaging.\n",
    "\n",
    "Boser, B., Guyon, I., Vapnik, V. (1992). A training algorithm for optimal margin classifiers. Proceedings of the fifth\n",
    "annual workshop on Computational learning theory. Ed. 07.1992, 144–152.\n",
    "\n",
    "Christianini, N., Ricci, E. (2008). Support Vector Machines. Encyclopedia of Algorithms, Springer.\n",
    "\n",
    "Deng, Guang & Cahill, L.W. (1993). An adaptive Gaussian filter for noise reduction and edge detection.\n",
    "Proc. of Nuclear Science Symposium and Medical Imaging Conference. 3. 1615 - 1619 vol.3.\n",
    "\n",
    "Dunn, K.W., Fu, C., Ho, D.J., Lee S., Han S., Salama P., Delp E. (2019). DeepSynth: Three-dimensional nuclear segmentation of\n",
    "biological images using neural networks trained with synthetic data. Sci Rep 9, 18295\n",
    "\n",
    "Evgeniou, T., Pontil, M. (2001). Support Vector Machines: Theory and Applications. Computer Science\n",
    "\n",
    "Johnson, R., Zhang, T. (2013). Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. 26th International\n",
    "Conference on Neural Information Processing Systems.\n",
    "\n",
    "Gedraite, E., Hadad, M. 2011. Investigation on the effect of a Gaussian Blur in image filtering and segmentation. 393-396.\n",
    "\n",
    "Geman, S., Bienenstock, E., Doursat, R. (1992). Neural Networks and the Bias/Variance Dilemma. Neural Computation.\n",
    "\n",
    "Guanasekaran, T., Shankar Kumar, K.R. (2010), Modified concentric circular micostrip array configurations for\n",
    "wimax base station. Journal of Theoretical and Applied Information Technology Vol 12. No. 1\n",
    "\n",
    "Hamill, P. (2005). Unit Test Frameworks: Tools for High-Quality Software Development (S. 1 f.).\n",
    "\n",
    "Mayer, N., Ilg, E., Fischer, P., Hazirbas, C., Cremers, D., Dosovitskiy, A.,Brox, T. (2018). What Makes Good Synthetic\n",
    "Training Data for Learning Disparity and Optical Flow Estimation?. Int J Comput Vis 126, 942–960.\n",
    "\n",
    "Mehta, B., Diaz, M., Golemo, F., Pal, C. J., Paull, L. (2020). Active Domain Randomization. Proceedings of Machine Learning Research.\n",
    "\n",
    "Menze, B., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi, L.,\n",
    "Gerstner, E., Weber, M., Arbel, T., Avants, B., Ayache, N., Buendia, P., Collins, D., Cordier, N., Corso, J., Criminisi, A., Das, T.,\n",
    "Delingette, H., Demiralp, Ç., Durst, C., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P., Guo, X., Hamamci, A.,\n",
    "Iftekharuddin, K., Jena, R., John, N., Konukoglu, E., Lashkari, D., Mariz, J., Meier, R., Pereira, S., Precup, D., Price, S., Raviv, T.,\n",
    "Reza, S., Ryan, M., Sarikaya, D., Schwartz, L., Shin, H., Shotton, J., Silva, C., Sousa, N., Subbanna, N., Szekely, G., Taylor, T.,\n",
    "Thomas, O., Tustison, N., Unal, G., Vasseur, F., Wintermark, M., Ye, D., Zhao, L., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., Van Leemput, K. (2015).\n",
    "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging.\n",
    "\n",
    "Mudrova, M.,  Prochazka A. (2005). Principal Component Analysis in Image Processing.\n",
    "\n",
    "Nandi, D., Ashour, A., Sourav, S., Chakraborty, S., Mohammed Abdel-Megeed, M., Dey, N. (2015).\n",
    "Principal Component Analysis in Medical Image Processing: A Study. International journal of Image Mining.\n",
    "\n",
    "Rastar, A. (2019). A Novel Pixel-Averaging Technique for Extracting Training Data from a Single Image, Used in ML-Based Image Enlargement.\n",
    "\n",
    "Ruder, S. (2017). An overview of gradient descent optimization algorithms. Insight Centre for Data Analytics.\n",
    "\n",
    "Suthaharan S. (2016). Support Vector Machine. Machine Learning Models and Algorithms for Big Data Classification.\n",
    "Integrated Series in Information Systems, vol 36. Springer.\n",
    "\n",
    "Thai, L.H., Tran S.H., Nguyen T.T. (2012). Image Classification using Support Vector Machine and Artificial Neural Network.\n",
    "International Journal of Information Technology and Computer Science (IJITCS).\n",
    "\n",
    "Tripathi, S., Chandra S., Agrawal, A., Tyagi, A., Rehg, J. M., Chari, V. (2019). Learning to Generate Synthetic\n",
    "Data via Compositing. IEEE Xplore.\n",
    "\n",
    "Vabalas, A., Gowen, E., Poliakoff, E., Casson, A.J. (2019). Machine learning algorithm validation with a limited sample size. Plos one.\n",
    "\n",
    "Ward, D.,Moghadam P.,Hudson, N. (2019). Deep Leaf Segmentation Using Synthetic Data. CoRR.\n",
    "\n",
    "Yang, Z., Yu, Y., You, Y., Steinhardt, J., Ma, Y. (2020). Rethinking Bias-Variance Trade-off\n",
    "for Generalization of Neural Networks. Cornell University.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}