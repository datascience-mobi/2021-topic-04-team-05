{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Report of Project 05: Implementation and Evaluation of Machine Learning (Support Vector Machine) Segmentation\n",
    "## Data analysis project - B.Sc. Molecular Biotechnology Heidelberg University\n",
    "### 19.07.21\n",
    "### Authors: Michelle Emmert, Juan Hamdan, Laura Sanchis Pla and Gloria Timm\n",
    "\n",
    "*ich werde kommentare, anmerkungen, noch zu klärende fragen... immer in kursiv schreiben*\n",
    "\n",
    "*Idee: unseren algorithmus anhand eines bildes erklären und dieses immer wieder im report zeigen um die veränderungen\n",
    "zu zeigen & am ende dice berechnen --> veranschaulichen*\n",
    "\n",
    "*Unterüberschriften noch einfügen*\n",
    "\n",
    "# Abstract\n",
    "\n",
    "Support Vector Machines, also known as SVMs, are supervised learning models with associated learning algorithms that\n",
    "analyze data  for classification and regression analysis.\n",
    "\n",
    "Their theoretical foundations and their experimental success encourage further research on their characteristics, as well\n",
    "as their further use. This report presents the summary of a project that implement and evaluate Support Vector Machine\n",
    "Segmentation for segmentation of cell nuclei using dice score, synthetically generated images, and pre-processing methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Table of contents *ggf. noch 2.1, 2.2 etc.*\n",
    "**1. Introduction** <br>\n",
    "**2. Our Data** <br>\n",
    "**3.** <br>\n",
    "**4. Pre-processing** <br>\n",
    "**5. Implementation of the support vector machine** <br>\n",
    "**6. Evaluation using the Dice coefficient** <br>\n",
    "**6.1 The Theory Behind the Dice Coefficient** <br>\n",
    "**6.2 Implementing the dice coefficient** <br>\n",
    "**6.3 Synthetic Images** <br>\n",
    "**6.4 PCA for feature selection for the SVM** <br>\n",
    "**6.5 Stochastic gradient decent to minimize the loss gradient** <br>\n",
    "**6.6 Cross-validation\n",
    "**7. Results** <br>\n",
    "**8. Discussion** <br>\n",
    "**9. Bibliography**\n",
    "\n",
    "\n",
    "# 1. Introduction\n",
    "Image segmentation is a process during which important features of a picture are extricated, to aid analysis and extraction\n",
    "of information. This is done by assigning labels to all pixels of the image. Pixels sharing defined traits, are allocated the same label.\n",
    "Nuclei segmentation is a subform of image segmentation. In medicine nuclei segmentation could be/is used to classify and\n",
    "grade cancer based on cancer histopathology. Today, cancer grading is still often done manually by visual analysis of tissue samples.\n",
    "Problematic with this method is its inter- and intra-observer variability regarding the gradings quality, its low reproducibility\n",
    "as well as its great time consumption. Cancer grading is an essential step in quantifying the degree of malignancy and thus\n",
    "to predict patient prognosis and prescribe a treatment.\n",
    "\n",
    "Using a support vector machine is one way nuclei segmentation can be achieved.\n",
    "We use the support vector machine to differentiate between foreground and background pixels. Foreground pixels are\n",
    "subsequently labeled as nuclei and displayed white. Background pixels are colored black. This also allows counting of the nuclei in the end.\n",
    "To efficiently segment our images with a support vector machine, they first need to undergo preprocessing, to enhance\n",
    "picture quality. After segmentation, we want to evaluate the quality of our segmentation, which we will be using the Dice\n",
    "score for.\n",
    "\n",
    "# 2. Our Data\n",
    "Our data consists of 3 datasets with a total of 28 very different kinds of images all showing nuclei.\n",
    "The pictures of the first dataset show GFP-transfected GOWT1 mouse embryonic stem cells. The second set of images show\n",
    "Histone-2B-GFP expressing HeLa cells, while our third set consists of pictures of mouse embryonic fibroblasts, in which\n",
    "CD-antigens were tagged with enhanced-GFP.\n",
    "Though all images are microscopic images of nuclei, the three sets vary greatly in other features.\n",
    "The images are of different formats (1024 x 1024, 1100 x 700, 1344 x 1024), were all acquired differently and display\n",
    "different numbers of nuclei: within a range of 15-65 nuclei per image.\n",
    "Additionally, all images pose diverse challenges for our image analysis.\n",
    "Firstly brightness and resolution of the images differ. On top of that every set of images has its individual challenges\n",
    "like white flashes, clustering of nuclei or nuclei leaving the image or undergoing mitosis.\n",
    "\n",
    "\n",
    "# 6. Dice Coefficient\n",
    "## 6.1 The Theory Behind the Dice Coefficient\n",
    "The Dice coefficient is a score to evaluate and compare the accuracy of a segmentation.\n",
    "Needed for its calculation are the segmented image, as well as a corresponding binary reference point also called\n",
    "ground truth (Bertels et al., 2019).\n",
    "As a ground truth image, researchers mostly use the segmentation result of humans. We will use the ground truth images\n",
    "provided with our data sets, which we suspect to be acquired by this method.\n",
    "Using the ground truth image, the labels true positive (TP), false positive (FP) and false\n",
    "negative (FN) are assigned to each pixel of the segmented image (Menze et al., 2015).\n",
    "This information is then used to calculate the dice coefficient using formula (1):"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(1) \\ dice = {\\frac{2TP}{2TP + FP + FN}} \\ \\ \\varepsilon \\ \\ [0,1]\n",
    "\\end{equation} <br>\n",
    "(Menze et al., 2015)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A dice score of 0 indicates that the ground truth and the segmentation result do not have any overlap. A dice score of 1 on the\n",
    "other hand, shows a 100% overlap of ground truth and segmented image (Bertels et al., 2019).\n",
    "\n",
    "\n",
    "## 6.2 Implementing the dice coefficient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Finalmodules.dicescore'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-c79ffd81d436>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mFinalmodules\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdicescore\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mdicescore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'Finalmodules.dicescore'"
     ]
    }
   ],
   "source": [
    "import dicescore_code.dicescore as dicescore\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Synthetic Images\n",
    "### 6.3.1 Definition and Goal\n",
    "The concept behind creating synthetic images, is to use algorithms and images, which are already available to generate\n",
    "new ones. (Dunn et al., 2019) Although our first objective was to just use these new images to test our code for the dice score, we realized\n",
    "while researching for this topic that synthetic images have an immense potential, most of all for the training of machine\n",
    "learning algorithms. By expanding our training set with diverse images of good quality, we expect a more accurate model (Mayer et al., 2017 ALTERNATIV: https://arxiv.org/abs/1807.07428).\n",
    "In order to train our model and test it afterwards with new data, we have to split up our dataset of 28 cell images\n",
    "between those two sets, which leads to a training set of less than 28 images. Because of this, we decided to implement\n",
    "our synthetically produced images not only to test the dice score, but to enlarge our training data pool for our\n",
    "Support Vector Machine, and afterwards, check if its efficiency was improved with our dice score. There are many methods\n",
    "that can be used in order to generate synthetic images (Ward et al., 2019). Because of the scope of our project and the\n",
    "kind of images we want to produce, we focused on image composition and domain transfer.\n",
    "\n",
    "### 6.3.2 Image Composition\n",
    "Image composition consists of taking various foreground images, which have been segmented out of their backgrounds or\n",
    "have a .png format to begin with, and paste them onto different backgrounds (Tripathi, 2019). The foreground images can\n",
    "be modified using different light conditions, contrasts, zooms or rotations in order to achieve more variety in the results\n",
    "(Ward et al., 2019; Alghonaim and Johns, 2020).\n",
    "--> probably not as useful for our case as cells are usually in front of dark background, but still an option to evaluate\n",
    "--> will the Dice Score get better with that method?\n",
    "\n",
    "*Here we could insert the code from my Jupyter notebook -> issue: it hasn't been written by us\n",
    "(and it hasn't been modified either...), so maybe it would be better to just use our own code when we write it.*\n",
    "\n",
    "### 6.3.3 Domain randomization\n",
    "In domain randomization there is a model from the object class you want to train your model for, and in that model,\n",
    "every parameter from the object and its environment that is not necessary for its recognition by the machine has been\n",
    "randomized (Mehta et al., 2020). This means for example size, lighting or color, and there are very powerful tools to do this, like Unity or\n",
    "Blender (Ward et al., 2019; Alghonaim and Johns, 2020). <br>\n",
    "*--> probably more useful, as we have changes in size, dividing or leaving cells etc. However, it is usually\n",
    "used to train robots to work from a simulation to reality, so it might be a bit too much.*\n",
    "\n",
    "# Support Vector Machine\n",
    "In 1992, Vapnik and coworkers proposed a supervised algorithm, developed from statistical learning, to solve classification\n",
    "problems (Vapnik et al., 1992). Since then, their machine learning method evolved into what is now known as\n",
    "Support Vector Machines (SVM): a class of algorithms for classification, regression and other applications that\n",
    "represent the current state of the art in the field (Suthaharan, 2016; Guanasekaran, 2010; Christianini and Ricci, 2008).\n",
    "\n",
    "By providing a training data set with binary labels, the SVM is able to learn how to classify data points using certain\n",
    "features. This capability can subsequently be used to classify new data, called test data, using its features (Thai et. al, 2012).\n",
    "SVMs have been successfully applied to a number of applications, ranging from the time series prediction and face recognition\n",
    "to biological data processing for medical diagnosis (Evgeniou, 2001).\n",
    "In image processing, support vector machines are used for one of the classical challenges: image classification (Evgeniou, 2001).\n",
    "\n",
    "## The Mathematical Background\n",
    "\n",
    "The mathematical concepts are the key in understanding how support vector machines work.\n",
    "The goal of a support vector machine is to seperate data points into two groups of provided labels with an optimal\n",
    "hyperplane.\n",
    "This hyperplane is described by"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(2) \\ w * x + b = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and fullfilling the following condition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(3) \\ h =\n",
    "\\left\\{\n",
    "  \\begin{aligned}\n",
    "    +1 \\ \\ if \\ \\ w⋅x_i +b≥ +1 - \\varepsilon_i\\\\\n",
    "    -1 \\ \\ if \\ \\ w⋅x_i +b< -1 - \\varepsilon_i\n",
    "  \\end{aligned}\n",
    "  \\right.\n",
    "  \\ \\ \\varepsilon \\geqq 0 \\ \\forall_i \\ ; \\ i=1...m\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "whilst for two dimensions $w = (a, -1)$, whereas $a$ is the slope of the line, and $x = (x_1, x_2)$ and represents a\n",
    "data point. $\\varepsilon$ is a variable, representing the inaccuracy of the hyperplane. It is added to the constraint to\n",
    "prevent overfitting of the model onto the training set. Without $\\varepsilon$ the geometric margin M is called a hard margin,\n",
    "as it does not allow data points of one group to be falsely labeled as members of the other group. This does not result in\n",
    "the best model, as single falsely assigned data points, can have a lower impact on the quality of the model, than a suboptimal\n",
    "hyperplane. Therefore, $\\varepsilon$ is introduced and therefore M named a soft margin.\n",
    "\n",
    "To choose the optimal hyperplane we want to minimize the margin as follows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(4)\\ M = min_{i=1...m} \\ y_i(\\frac{w}{||w||}*x + \\frac{b}{||w||})\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The largest margin M out of all margins computed in our training phase will be selected. The variables w and b are\n",
    "divided through the length of the vector w calculated with the Euclidean norm formula, as they have to be scale invariant.\n",
    "These variables w and b are the ones to be found a value for.\n",
    "\n",
    "This leads us to the following optimization problem.\n",
    "We want to maximize M:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(5) \\ max_{w,b,\\varepsilon} M\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This maximization problem is equivalent to"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(6) \\ max_{w,b,\\varepsilon} \\frac{1}{||w||} \\ + \\ \\sum^{m}_{i=1}\\varepsilon_i\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and can be rewritten as the following minimalization problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(7) \\ min_{w,b,\\varepsilon}  \\ \\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i \\\\\n",
    "subject \\ to \\ \\  y_i(w⋅x_i+b)≥1− \\varepsilon_i \\ , \\varepsilon \\geqq 0 \\ \\forall_i \\ , \\ i=1...m\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The regularization parameter C is chosen by the user and determines the weight of $\\varepsilon$.\n",
    "The larger C is, the higher the penalty for errors and therefore the harder the margin.\n",
    "\n",
    "In order to solve this constrained optimization problem, in which we want to maximize the margin while fulfilling our\n",
    " conditions or constraints, Lagrange multipliers are used. The principle behind it, is that at the optimum, the gradient\n",
    " of our objective function is parallel or antiparallel to the gradient of the constraint function. Because of this,\n",
    " they have to be equal or a multiple of each other, which is what the Lagrange multiplier is showcasing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(8) \\ \\nabla f(x) - \\alpha \\nabla g(x) = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we insert our functions, we get the following Lagrangian function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(9) \\ f(x)= \\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i\n",
    "\\\\(10) \\ g(x) = y_i(w⋅x_i+b) - 1 + \\varepsilon_i\n",
    "\\\\(11) \\ \\mathcal{L}(w,b,\\alpha) =\\frac{1}{2}||w||^2\\ + \\ C\\sum^{m}_{i=1}\\varepsilon_i - \\sum^{m}_{i=1}\\alpha_i[y_i\n",
    "(w⋅x_i+b) - 1 + \\varepsilon_i]\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to solve this Lagrange problem, it is relaxed into a dual problem, where the constraints are incorporated\n",
    "into the function, so that it only depends on the Lagrange multipliers, making it easier to be solved. These are the\n",
    "two constraints for the dual problem:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "(12) \\ \\nabla_w \\mathcal{L}(w,b,\\alpha) = w - \\sum^{m}_{i=1} \\alpha_i y_i x_i = 0\n",
    "\\\\ (13) \\ \\nabla_b \\mathcal{L}(w,b,\\alpha) = - \\sum^{m}_{i=1} \\alpha_i y_i = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If they are inserted into the Lagrange function, this is the result for the dual problem.\n",
    "\\begin{equation}\n",
    "(14) \\ max_{\\alpha}  \\ \\sum^{m}_{i=1}\\alpha_i - \\frac{1}{2}\\sum^{m}_{i=1}\\sum^{m}_{j=1}\\alpha_i\\alpha_j y_i y_j x_i · x_j \\\\\n",
    "subject \\ to \\ \\ 0≤\\alpha_i≤C, i=1...m, \\sum^{m}_{i=1}\\alpha_iy_i = 0\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above equation it becomes clear that its maximization only depends on the dot product of the support vectors\n",
    "$x_i · x_j$. This becomes an advantage when the data are not linearly separable. The trick is to transform the data\n",
    "into a higher dimension, where a hyperplane can be found that separates it. However, if the dataset is very big,\n",
    "calculating the transformation would be very time-consuming. Because of this, the Kernel trick is used. It consists\n",
    "in a function which calculates the dot product $x_i · x_j$ as if it were in a higher dimension. There is the linear\n",
    "Kernel function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "K(x_i, x_j)=\\phi(x_i) · phi(x_j)\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "but it varies depending on the type of data that has to be classified. Other know and comonly used Kernel functions are:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{equation}\n",
    "K(x_i, x_j)=x_i · x_j,\n",
    "\\end{equation}\n",
    "\n",
    "!!!*still missing/ has to be elaborated*!!!:\n",
    "-Wolfe dual problem\n",
    "-Kernel trick"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.5 K-Fold Cross validation\n",
    "Validation is a widely used technique in data science to evaluate how stable a machine learning (ML) model is and to\n",
    "assess how well the model would generalize to new, independent data. Relevant for these two characteristics is the MLs\n",
    "ability to differentiate between relevant patterns and noise in the data available (Vabalas et. al, 2019). As a measure for how good the ML is\n",
    "able to achieve this, the bias-variance tradeoff can be used (Geman et. al, 1992; Berrar, 2019).\n",
    "Bias and variance are both sources of error in ML generalization. With increasing model complexity, bias decreases and\n",
    "variance increases. In short: High bias indicates 'underfitting', high variance points at an 'overfitted' model.\n",
    "'Underfitting' describes the performance a model, which is neither able to classify its training data nor new data well,\n",
    "because it captures too little patterns. 'Overfitting' on the other hand refers to a model, that is overly sensitive to\n",
    "noise and random pattern in it's training data and for that reason performs poorly on new data. Optimally both bias and\n",
    "variance could be minimized (Geman et. al, 1992; Berrar, 2019). In reality the right balance of both is needed to generate an optimal model. (QUELLE)\n",
    "\n",
    "One validation technique is k-fold cross validation (CV).\n",
    "In CV, the data available is split into k subsets. The data encompasses n dissimilar samples. k is a random\n",
    "integer between 1 and n. For each iteration k-1 subsets are used as training data, while the remaining subsets are\n",
    "used to test the model. To put it differently: each data sample is part of the testing data once and part of the training\n",
    "data for all other iterations (Vabalas et. al, 2019).\n",
    "\n",
    "As our data consists of only 28 images, we used a special case of cross-validation called leave one out cross-validation (LOOCV).\n",
    "In this approach k equals the number of samples.\n",
    "\n",
    "*This approach substantially reduces bias, as it uses all data points. Simultaneously variance also decreases,\n",
    "because YXXXX. But as only one datapoint is used for testing in each iteration, higher variation in testing model effectiveness\n",
    "can occur.* (QUELLE)\n",
    "\n",
    "#https://www.statology.org/bias-variance-tradeoff/\n",
    "#https://www.researchgate.net/profile/Daniel-Berrar/publication/324701535_Cross-Validation/links/5cb4209c92851c8d22ec4349/Cross-Validation.pdf\n",
    "\n",
    "**9. Bibliography**\n",
    "Alghonaim, R., Johns, E. (2020).Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. CoRR.\n",
    "\n",
    "Berrar, D. (2019). Cross-validation. Data Science Laboratory, Tokyo Institute of Technology.\n",
    "\n",
    "Bertels, J., Eelbode, T., Berman, M., Vandermeulen D., Maes F., Bisschops, R., Blaschko, M. (2020).\n",
    "Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index.\n",
    "IEEE Trans Med Imaging.\n",
    "\n",
    "Boser, B., Guyon, I., Vapnik, V. (1992). A training algorithm for optimal margin classifiers. Proceedings of the fifth\n",
    "annual workshop on Computational learning theory. Ed. 07.1992, 144–152.\n",
    "\n",
    "Christianini, N., Ricci, E. (2008). Support Vector Machines. Encyclopedia of Algorithms, Springer.\n",
    "\n",
    "Dunn, K.W., Fu, C., Ho, D.J., Lee S., Han S., Salama P., Delp E. (2019). DeepSynth: Three-dimensional nuclear segmentation of\n",
    "biological images using neural networks trained with synthetic data. Sci Rep 9, 18295\n",
    "\n",
    "Evgeniou, T., Pontil, M. (2001). Support Vector Machines: Theory and Applications. Computer Science\n",
    "\n",
    "Guanasekaran, T., Shankar Kumar, K.R. (2010), Modified concentric circular micostrip array configurations for\n",
    "wimax base station. Journal of Theoretical and Applied Information Technology Vol 12. No. 1\n",
    "\n",
    "Mayer, N., Ilg, E., Fischer, P., Hazirbas, C., Cremers, D., Dosovitskiy, A.,Brox, T. (2018). What Makes Good Synthetic\n",
    "Training Data for Learning Disparity and Optical Flow Estimation?. Int J Comput Vis 126, 942–960.\n",
    "\n",
    "Mehta, B., Diaz, M., Golemo, F., Pal, C. J., Paull, L. (2020). Active Domain Randomization. Proceedings of Machine Learning Research.\n",
    "\n",
    "Menze, B., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi, L.,\n",
    "Gerstner, E., Weber, M., Arbel, T., Avants, B., Ayache, N., Buendia, P., Collins, D., Cordier, N., Corso, J., Criminisi, A., Das, T.,\n",
    "Delingette, H., Demiralp, Ç., Durst, C., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P., Guo, X., Hamamci, A.,\n",
    "Iftekharuddin, K., Jena, R., John, N., Konukoglu, E., Lashkari, D., Mariz, J., Meier, R., Pereira, S., Precup, D., Price, S., Raviv, T.,\n",
    "Reza, S., Ryan, M., Sarikaya, D., Schwartz, L., Shin, H., Shotton, J., Silva, C., Sousa, N., Subbanna, N., Szekely, G., Taylor, T.,\n",
    "Thomas, O., Tustison, N., Unal, G., Vasseur, F., Wintermark, M., Ye, D., Zhao, L., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., Van Leemput, K. (2015).\n",
    "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging.\n",
    "\n",
    "Suthaharan S. (2016). Support Vector Machine. Machine Learning Models and Algorithms for Big Data Classification.\n",
    "Integrated Series in Information Systems, vol 36. Springer\n",
    "\n",
    "Geman, S., Bienenstock, E., Doursat, R. (1992). Neural Networks and the Bias/Variance Dilemma. Neural Computation.\n",
    "\n",
    "Thai, L.H., Tran S.H., Nguyen T.T. (2012). Image Classification using Support Vector Machine and Artificial Neural Network.\n",
    "International Journal of Information Technology and Computer Science(IJITCS)\n",
    "\n",
    "Tripathi, S., Chandra S., Agrawal, A., Tyagi, A., Rehg, J. M., Chari, V. (2019). Learning to Generate Synthetic\n",
    "Data via Compositing. IEEE Xplore.\n",
    "\n",
    "Vabalas, A., Gowen, E., Poliakoff, E., Casson, A.J. (2019). Machine learning algorithm validation with a limited sample size. Plos one.\n",
    "\n",
    "Ward, D.,Moghadam P.,Hudson, N. (2019). Deep Leaf Segmentation Using Synthetic Data. CoRR.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}