{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Report of Project 05: Implementation and Evaluation of Machine Learning (Support Vector Machine) Segmentation\n",
    "## Data analysis project - B.Sc. Molecular Biotechnology Heidelberg University\n",
    "### 19.07.21\n",
    "### Authors: Michelle Emmert, Juan Hamdan, Laura Sanchis and Gloria Timm\n",
    "\n",
    "*ich werde kommentare, anmerkungen, noch zu klärende fragen... immer in kursiv schreiben*\n",
    "\n",
    "*Idee: unseren algorithmus anhand eines bildes erklären und dieses immer wieder im report zeigen um die veränderungen\n",
    "zu zeigen & am ende dice berechnen --> veranschaulichen*\n",
    "\n",
    "*Unterüberschriften noch einfügen*\n",
    "\n",
    "# Abstract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Table of contents *ggf. noch 2.1, 2.2 etc.*\n",
    "**1. Introduction** <br>\n",
    "**2. Our Datasets** <br>\n",
    "**3.** <br>\n",
    "**4. Pre-processing** <br>\n",
    "**5. Implementation of support vector machine** <br>\n",
    "**6. Evaluation using the Dice coefficient** <br>\n",
    "**7. Results** <br>\n",
    "**8. Discussion** <br>\n",
    "**9. Bibliography**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. Dice Coefficient\n",
    "## 6.1 The Theory Behind the Dice Coefficient\n",
    "The Dice coefficient is a score to evaluate and compare the accuracy of a segmentation.\n",
    "Needed for its calculation are the segmented image, as well as a corresponding binary reference point also called\n",
    "ground truth (Bertels et al., 2019).\n",
    "As a ground truth image, researchers mostly use the segmentation result of humans. We will use the ground truth images\n",
    "provided with our data sets, which we suspect to be acquired by this method.\n",
    "Using the ground truth image, the labels true positive (TP), false positive (FP) and false\n",
    "negative (FN) are assigned to each pixel of the segmented image (Menze et al., 2015).\n",
    "This information is then used to calculate the dice coefficient using formula (1):\n",
    "\n",
    "(1) dice = ${\\frac{2TP}{2TP + FP + FN}}$ \\varepsilon [0,1] <br>\n",
    "(Menze et al., 2015)\n",
    "\n",
    "A dice score of 0 indicates that the ground truth and the segmentation result do not have any overlap. A dice score of 1 on the\n",
    "other hand, shows a 100% overlap of ground truth and segmented image (Bertels et al., 2019).\n",
    "\n",
    "## 6.2 Implementing the dice coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Finalmodules.dicescore'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-c79ffd81d436>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mFinalmodules\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdicescore\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mdicescore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'Finalmodules.dicescore'"
     ]
    }
   ],
   "source": [
    "import Finalmodules.dicescore as dicescore\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.3 Synthetic Images\n",
    "### 6.3.1 Definition and Goal\n",
    "The concept behind creating synthetic images, is to use algorithms and images, which are already available to generate\n",
    "new ones. (Dunn et al., 2019) Although our first objective was to just use these new images to test our code for the dice score, we realized\n",
    "while researching for this topic that synthetic images have an immense potential, most of all for the training of machine\n",
    "learning algorithms. By expanding our training set with diverse images of good quality, we expect a more accurate model (Mayer et al., 2017 ALTERNATIV: https://arxiv.org/abs/1807.07428).\n",
    "In order to train our model and test it afterwards with new data, we have to split up our dataset of 28 cell images\n",
    "between those two sets, which leads to a training set of less than 28 images. Because of this, we decided to implement\n",
    "our synthetically produced images not only to test the dice score, but to enlarge our training data pool for our\n",
    "Support Vector Machine, and afterwards, check if its efficiency was improved with our dice score. There are many methods\n",
    "that can be used in order to generate synthetic images (Ward et al., 2019). Because of the scope of our project and the\n",
    "kind of images we want to produce, we focused on image composition and domain transfer.\n",
    "\n",
    "### 6.3.2 Image Composition\n",
    "Image composition consists of taking various foreground images, which have been segmented out of their backgrounds or\n",
    "have a .png format to begin with, and paste them onto different backgrounds (Tripathi, 2019). The foreground images can\n",
    "be modified using different light conditions, contrasts, zooms or rotations in order to achieve more variety in the results (???).\n",
    "--> probably not as useful for our case as cells are usually in front of dark background, but still an option to evaluate\n",
    "--> will the Dice Score get better with that method?\n",
    "\n",
    "*Here we could insert the code from my Jupyter notebook -> issue: it hasn't been written by us\n",
    "(and it hasn't been modified either...), so maybe it would be better to just use our own code when we write it.*\n",
    "\n",
    "### 6.3.3 Domain randomization\n",
    "In domain randomization there is a model from the object class you want to train your model for, and in that model,\n",
    "every parameter from the object and its environment that is not necessary for its recognition by the machine has been\n",
    "randomized. This means for example size, lighting or color, and there are very powerful tools to do this, like Unity or\n",
    "Blender. <br>\n",
    "*--> probably more useful, as we have changes in size, dividing or leaving cells etc. However, it is usually\n",
    "used to train robots to work from a simulation to reality, so it might be a bit too much.*\n",
    "\n",
    "# Support Vector Machine\n",
    "\n",
    "1. Tensorflow --> an Option?\n",
    "\n",
    "\n",
    "**9. Bibliography**\n",
    "Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J, Burren Y, Porz N, Slotboom J, Wiest R, Lanczi L,\n",
    "Gerstner E, Weber MA, Arbel T, Avants BB, Ayache N, Buendia P, Collins DL, Cordier N, Corso JJ, Criminisi A, Das T,\n",
    "Delingette H, Demiralp Ç, Durst CR, Dojat M, Doyle S, Festa J, Forbes F, Geremia E, Glocker B, Golland P, Guo X, Hamamci A,\n",
    "Iftekharuddin KM, Jena R, John NM, Konukoglu E, Lashkari D, Mariz JA, Meier R, Pereira S, Precup D, Price SJ, Raviv TR,\n",
    "Reza SM, Ryan M, Sarikaya D, Schwartz L, Shin HC, Shotton J, Silva CA, Sousa N, Subbanna NK, Szekely G, Taylor TJ,\n",
    "Thomas OM, Tustison NJ, Unal G, Vasseur F, Wintermark M, Ye DH, Zhao L, Zhao B, Zikic D, Prastawa M, Reyes M, Van Leemput K.\n",
    "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging. 2015\n",
    "\n",
    "\n",
    "Bertels, J., Eelbode, T., Berman, M., Vandermeulen D., Maes F., Bisschops, R., Blaschko, M. 2019.\n",
    "Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index\n",
    "\n",
    "\n",
    "Dunn, K.W., Fu, C., Ho, D.J., Lee S., Han S., Salama P., Delp E. DeepSynth: Three-dimensional nuclear segmentation of\n",
    "biological images using neural networks trained with synthetic data. Sci Rep 9, 18295 (2019)\n",
    "\n",
    "\n",
    "Mayer, N., Ilg, E., Fischer, P., Hazirbas, C., Cremers, D., Dosovitskiy, A.,Brox, T.  What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?.\n",
    "Int J Comput Vis 126, 942–960 (2018).\n",
    "\n",
    "\n",
    "Ward, D.,Moghadam P.,Hudson, N. Deep Leaf Segmentation Using Synthetic Data\n",
    "(2019) *welches journal?*\n",
    "\n",
    "Tripathi, S., Chandra S., Agrawal, A., Tyagi, A., Rehg, J. M., Chari, V. (2019) Learning to Generate Synthetic\n",
    "Data via Compositing *welches journal?*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}